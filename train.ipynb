{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef989636-334a-4ef4-a327-4762d8dfafaf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div style=\"magin-top:-10%;margin-left:10%\">\n",
    "<h1 style=\"background-color:#a31f34;color:#eeeeee;margin-right:10%;border-radius:15px\">\n",
    "    Building a Production ML Pipeline For Gravitational Wave Detection\n",
    "</h1>\n",
    "\n",
    "<div style=\"color: #333333;font-size: 24pt\">Alec Gunny$^{*,1}$, Ethan Marx$^1$, William Benoit$^2$, Deep Chatterjee$^1$, Michael Coughlin$^2$, Katya Govorkova$^1$, Philip Harris$^1$, Erik Katsavounidis$^1$, Eric Moreno$^1$, Rafia Omer$^2$, Ryan Raikman$^1$, Muhammed Saleem$^2$</div>\n",
    "\n",
    "\n",
    "<p style=\"font-size:14pt;padding-top:3%\">\n",
    "<sub><sup>1 - Massachussetts Institute of Technology</sup></sub>\n",
    "</p>\n",
    "<p style=\"font-size:14pt;margin-top:-1%\">\n",
    "<sub><sup>2 - University of Minnesota</sup></sub>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d5374e0-18dd-49f7-9367-fdb6aa9dce6c",
   "metadata": {
    "editable": true,
    "rise": {
     "backimage": "https://www.ligo.caltech.edu/system/avm_image_sqls/binaries/58/page/Gravity_Waves_StillImage.jpg?1465865066",
     "footer": "https://www.ligo.caltech.edu"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-image: url(https://www.ligo.caltech.edu/system/avm_image_sqls/binaries/58/page/Gravity_Waves_StillImage.jpg?1465865066); background-repeat: no-repeat; background-size: cover;height: auto; width: 100%;color: #dedede\";margin-top:-100px>\n",
    "    <h1 style=\"color: #dedede\">Gravitational Waves</h1>\n",
    "    <table style=\"background: rgba(255,255,255,0.0);color: #dedede;margin-top:-2%\">\n",
    "        <tr style=\"background: rgba(255,255,255,0.0)\">\n",
    "            <td style=\"background: rgba(255,255,255,0.0);position: relative;top: -20px;font-size:32px\">\n",
    "                <li>Large-scale astrophysical events ripple fabric of spacetime, detected via earth-based interferometers</li>\n",
    "                <li>Low-latency detection of events allows for localization and follow-up</li>\n",
    "                <li>ML algorithms - small inference compute</li>\n",
    "                <li>Binary blackhole mergers are an \"easy\" problem to prove capabilities and build infra</li>\n",
    "            </td>\n",
    "            <td style=\"background: rgba(255,255,255,0.0)\">\n",
    "                <span style=\"font-size: 24px\">Focusing here on LIGO inteferometers</span>\n",
    "                <p></p>\n",
    "                <img src=\"https://www.ligo.caltech.edu/system/avm_image_sqls/binaries/52/page/HiResHanford_5.jpg?1465343234\" width=\"65%\"></img>\n",
    "                <figcaption style=\"margin-top:-5%\">Hanford, Washington</figcaption>\n",
    "                <p></p>\n",
    "                <img src=\"https://www.ligo.caltech.edu/system/avm_image_sqls/binaries/30/page/ligo-livingston-aerial-02.jpg?1447107179\" width=\"65%\"></img>\n",
    "                <figcaption style=\"margin-top:-5%\">Livingston, Louisiana</figcaption>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </table>\n",
    "\n",
    "<div class=\"footer\">https://www.ligo.caltech.edu</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb2188-e1a6-4b3f-ae1d-92e917603b6a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## ML in practice\n",
    "- Writing ML training code is easy: `model.fit(X, y)`\n",
    "- Doing good _science_ with ML is hard\n",
    "    - Largest gains from development of domain specific ecosystems\n",
    "    - Fast tools with intuitive APIs that map on to familiar concepts\n",
    "- GW physics has a great software ecosystem\n",
    "    - Robust simulations, good priors, etc., but none optimized for ML\n",
    "- Introduce here two libraries to seed such an ecosystem\n",
    "\n",
    "<a href=\"https://github.com/ML4GW/ml4gw\"><img src=\"https://cdn-icons-png.flaticon.com/512/25/25231.png\" width=4% style=\"vertical-align: middle;margin-right: 2%\">`ml4gw`</a> <span style=\"font-size: 24px;font-style: italic\"> - PyTorch training utilities</span><a href=\"https://github.com/ML4GW/hermes\" style=\"margin-left: 2%\"><img src=\"https://cdn-icons-png.flaticon.com/512/25/25231.png\" width=4% style=\"vertical-align: middle;margin-right: 2%\">`hermes`</a><span style=\"font-size: 24px;font-style: italic\"> - Inference-as-a-Service deployment utilities</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4b090",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### ML4GW\n",
    "- GitHub organization containing libraries, projects, etc.\n",
    "- Always room for more collaborators!\n",
    "\n",
    "<img src=\"https://chart.googleapis.com/chart?cht=qr&chl=https%3A%2F%2Fgithub.com%2FML4GW&chs=180x180&choe=UTF-8&chld=L|2\" rel=\"nofollow\" alt=\"qr code\">\n",
    "<figcaption>https://github.com/ML4GW</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772bd96-fcc3-4573-a7ff-8a51d3daf4c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Some implementation notes\n",
    "- Not live, but run in one fell swoop ([try it!](https://github.com/alecgunny/adass-2023-ml4gw-demo))\n",
    "- **NOT** about\n",
    "    - How to train an ML model\n",
    "    - `torch` or `lightning` or `gwpy` or `pycbc` or `bokeh`\n",
    "    - Even really `ml4gw` or `hermes`\n",
    "- **IS** about demonstrating how domain-optimized tools can deliver more robust models and systems\n",
    "    - Hide or skim details where necessary to stay focused\n",
    "- \"What about...\" Great question! Good tools let us answer good questions with more confidence\n",
    "- Full paper on BBH detection coming, stay posted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c76457-cf12-4954-9aea-c65fe4f29647",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Some implementation notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a2fe9-f6bd-425c-a5b5-c79faca12f7b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Coding in slides makes you space conscious, so I'll get some imports and constant definitions out of the way here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03530081-9e0e-4497-b3c5-7c63add68265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from utils import plotting\n",
    "\n",
    "ifos = [\"H1\", \"L1\"]\n",
    "sample_rate = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3c535-1122-4dea-9963-bd3abcc672e9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3 minute crash course in gravitational wave data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61bf6e-c3ce-42fe-b5b2-d2785e3f9e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ce801-e319-433e-bca0-0414f2d26802",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "`background.hdf5`\n",
    "\n",
    "Real, open data observed by the Hanford (H1) and Livingston (L1) interferometers between April 1st and April 22nd 2019\n",
    "- 1 week for training/validation, 2 weeks for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653e01d-15c9-481a-bc5a-c2e58639681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"data/background.hdf5\", \"r\") as f:\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        num_segments = len(f[split])\n",
    "        duration = sum([len(v[ifos[0]]) / sample_rate for v in f[split].values()])\n",
    "        print(\n",
    "            \"{} segments in {} split, corresponding to {:0.2f} h of livetime\".format(\n",
    "                num_segments, split, duration / 3600\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c900a0b4-5df1-4a8d-aefb-87db286559e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "`background.hdf5`\n",
    "\n",
    "What's this data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8f5c05-5684-406e-86c1-c53b625ecf47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"data/background.hdf5\", \"r\") as f:\n",
    "    dataset = f[\"train\"][\"1238175433-17136\"]  # start timestamp of segment-duration\n",
    "    background = {i: dataset[i][:sample_rate * 10] for i in ifos} # plot first 10s\n",
    "t = np.arange(sample_rate * 10) / sample_rate\n",
    "plotting.plot_timeseries(t, **background)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e458a99-d68a-4ada-a42d-83abf4118c36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "`signals.hdf5`\n",
    "\n",
    "Simulated waveforms generated by gravitational wave events along with the parameters of those events\n",
    "- Train and validation datasets contain raw waveform **polarizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0433085c-c82b-4e42-950b-2f12649eee76",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"data/signals.hdf5\") as f:\n",
    "    for split in [\"train\", \"valid\"]:\n",
    "        dataset = f[split][\"polarizations\"][\"cross\"]\n",
    "        num_signals, size = dataset.shape\n",
    "        print(\"{} {} s signals in {} split\".format(len(dataset), size/sample_rate, split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb6ea10-dbf2-4696-997a-36fb6c3e864c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "`signals.hdf5`\n",
    "\n",
    "And what do these signals look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ecbe84-c372-42e8-8a0a-2662faa14c29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"data/signals.hdf5\") as f:\n",
    "    # let's look at a loud training signal\n",
    "    split = f[\"train\"]\n",
    "    idx = split[\"parameters\"][\"snr\"][:].argmax()\n",
    "    signal = {i: split[\"polarizations\"][i][idx] for i in [\"cross\", \"plus\"]}\n",
    "t = np.arange(size) / sample_rate\n",
    "plotting.plot_timeseries(t, **signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9d1341-b13f-4a0e-aa4d-0221ee7221a5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "- Inteferometers act as antennae that respond linearly to polarizations\n",
    "- Function of relative locations/orientations of detectors and source\n",
    "\n",
    "<img src=\"images/orientations.png\" width=50% style=\"display: block;margin-left:auto;margin-right:auto\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455fac3f-598c-4972-9408-48799c68fc34",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pycbc.detector import Detector\n",
    "\n",
    "sky_params = [\"declination\", \"right_ascension\", \"polarization\"]\n",
    "with h5py.File(\"data/signals.hdf5\", \"r\") as f:\n",
    "    params = {i: f[f\"train/parameters/{i}\"][idx] for i in sky_params}\n",
    "\n",
    "responses = {}\n",
    "for ifo in ifos:\n",
    "    detector = Detector(ifo)\n",
    "    fp, fc = detector.antenna_pattern(t_gps=1238175433, **params)\n",
    "    response = ht = fp * signal[\"plus\"] + fc * signal[\"cross\"]  # call observed strain h(t)\n",
    "    responses[ifo] = ht\n",
    "plotting.plot_timeseries(t, **responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fe2773-4252-448b-bb13-81504d0802b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "- Projection from polarization $\\rightarrow$ response introduces\n",
    "    - phase shifts due to differences in arrival times\n",
    "    - differences in relative amplitudes due to slight differences in polarization\n",
    "- Test signals have been pre-projected so that we can analyze it as if it was a real event\n",
    "- All signals have been rejection sampled to ensure their **signal-to-noise ratio** (SNR) is $\\geq$ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f5634-43d7-4d93-bf6b-0bfa814fd853",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Back to detection\n",
    "What examples can we give a neural network that will help it learn to detect presence of signal?\n",
    "- \"Loudest\" part of signal is in last 0.5-1 seconds, near the **coalescence**\n",
    "- Signals add simply to background noise, i.e. $h(t) = n(t) + s(t)$\n",
    "- So why don't we:\n",
    "    - Take short windows of background, say 1s\n",
    "    - Add simulated/projected signals to ~50% of them\n",
    "    - Train a binary classification network on this data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f5dd7-49cf-427a-b10a-eeabd222e74d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "What would this data look like? What does the network \"see\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23dadb-a220-4902-bcf0-be64f2295f42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "injected, uninjected = {}, {}\n",
    "for ifo in ifos:\n",
    "    # I'm actually going to grab 2 seconds for reasons that will become clear momentarily\n",
    "    bg = background[ifo][-2 * sample_rate:]\n",
    "    uninjected[ifo] = bg\n",
    "    injected[ifo] = bg.copy()\n",
    "    injected[ifo][:sample_rate] += responses[ifo][-sample_rate:]\n",
    "t = np.arange(len(bg)) / sample_rate\n",
    "plotting.plot_side_by_side(uninjected, injected, t, titles=[\"Before injection\", \"After injection\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6956a604-253a-4c2b-ba39-6053160f681a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "No major difference - but this shouldn't be suprising:  background strain is $\\mathcal{O}(10^{-19})$, waveforms are $\\mathcal{O}(10^{-22})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f64b3-0e3d-41b9-9a63-7ee1a9c63bc8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Well we tried"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3337e1a0-c663-4b03-a13b-0bfccebc76ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff0ae4-2e72-4425-aae8-af2383b7f545",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "- Most of background is low frequency content - 10-30Hz\n",
    "- Most of signal is in 60-500Hz\n",
    "- Emphasize signal by **whitening** the data: normalize frequency content by amplitude spectral density (ASD) of background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a80a14-4435-4321-a7df-fe7672cea2cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gwpy.timeseries import TimeSeries\n",
    "\n",
    "asd_length, fftlength = 8, 2\n",
    "asds = {}\n",
    "for ifo in ifos:\n",
    "    bg = TimeSeries(background[ifo], sample_rate=sample_rate)\n",
    "    asds[ifo] = bg.crop(0, asd_length).asd(fftlength, method=\"median\")\n",
    "plotting.plot_spectral(**asds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bffe6d7-4667-4da1-8c7f-24bbd146e65a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ifo in ifos:\n",
    "    for src in [injected, uninjected]:\n",
    "        x = TimeSeries(src[ifo], sample_rate=sample_rate)\n",
    "        x = x.whiten(asd=asds[ifo], fduration=1)  # fduration is response time of filter\n",
    "        x = x.crop(0.5, 1.5)  # edges are corrupted by filter settle-in\n",
    "        src[ifo] = x.value\n",
    "t = t[sample_rate // 2: -sample_rate // 2]\n",
    "plotting.plot_side_by_side(uninjected, injected, t, titles=[\"Before injection\", \"After injection\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318803cf-55ad-4788-8730-29cffc02bc11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "So a quick review of how we'll generate the samples on which to train our neural network:\n",
    "\n",
    "<img src=\"images/sample-flow.png\" width=50% style=\"display: block;margin-left:auto;margin-right:auto\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518beb58-dd30-403b-a628-ae6027940fa1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Training the network\n",
    "<img src=\"images/sample-flow.png\" width=50% style=\"display: block;margin-left:auto;margin-right:auto\"></img>\n",
    "\n",
    "How do we do this in practice?\n",
    "- Too much background to fit in memory at once\n",
    "- Not sure if we can do it in real-time\n",
    "- Start by generating fixed train and validation datasets up front, then fit on these"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e302bc2-7706-4502-ade6-2fa5f7320a36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "- `make_dataset` will take care of these steps using the traditional GW software stack\n",
    "- Don't need to worry about the details, but clock the throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a3621-92e5-4e9c-839f-93d42f377587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import make_dataset\n",
    "\n",
    "background_f = h5py.File(\"data/background.hdf5\")\n",
    "signal_f = h5py.File(\"data/signals.hdf5\")\n",
    "with background_f, signal_f:\n",
    "    datasets = {}\n",
    "    for split in [\"train\", \"valid\"]:\n",
    "        datasets[split] = make_dataset(\n",
    "            ifos,\n",
    "            background_f[split],\n",
    "            signal_f[split],\n",
    "            kernel_length=1,\n",
    "            fduration=1,\n",
    "            psd_length=8,\n",
    "            fftlength=2,\n",
    "            sample_rate=sample_rate,\n",
    "            highpass=32\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf56b6-400b-4ce4-bdb8-506dd0128d76",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if datasets:\n",
    "    with h5py.File(\"data/cache.hdf5\", \"a\") as f:\n",
    "        for split, value in datasets.items():\n",
    "            X, y = value\n",
    "            try:\n",
    "                dataset = f[split]\n",
    "            except KeyError:\n",
    "                dataset = f.create_group(split)\n",
    "            else:\n",
    "                dataset.pop(\"X\")\n",
    "                dataset.pop(\"y\")\n",
    "            dataset.create_dataset(\"X\", data=X)\n",
    "            dataset.create_dataset(\"y\", data=y)\n",
    "else:\n",
    "    with h5py.File(\"data/cache.hdf5\", \"r\") as f:\n",
    "        datasets = {k: (v[\"X\"][:], v[\"y\"][:]) for k, v in f.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1839893d-c756-4984-a01b-536e02b54b49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "What does our data look like now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06beaff5-b3fc-406a-b6d1-edc38fa1ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\", \"valid\"]:\n",
    "    X, y = datasets[split]\n",
    "    num_signal = (y == 1).sum()\n",
    "    num_background = (y == 0).sum()\n",
    "    print(\"{} samples of shape {} in {} split, {} signal and {} background\".format(\n",
    "        len(X), X.shape[1:], split, num_signal, num_background\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2512e9-e2af-4d58-bff9-b685e533463d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Start by defining a simple `lightning` model that will train a 1D ResNet architecture on our dataset.\n",
    "\n",
    "Details not really important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb8d16-5d0d-47fe-a356-cbe27c815eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lightning import pytorch as pl\n",
    "from torchmetrics.classification import BinaryAUROC\n",
    "from utils.nn import ResNet\n",
    "\n",
    "class VanillaDetectionModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.001,\n",
    "        batch_size: int = 1024,\n",
    "        max_fpr: float = 1e-2  # only measure ourselves on FPRs close to where we'll operate\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.nn = ResNet(len(ifos), layers=[2, 3, 4, 2])\n",
    "        self.metric = BinaryAUROC(max_fpr=max_fpr)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.nn(X)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        X, y = batch\n",
    "        y_hat = self(X)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        X, y = batch\n",
    "        y_hat = self(X)\n",
    "        self.metric.update(y_hat, y)\n",
    "        self.log(\"valid_auroc\", self.metric, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        parameters = self.nn.parameters()\n",
    "        optimizer = torch.optim.AdamW(parameters, self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            self.hparams.learning_rate,\n",
    "            pct_start=0.1,\n",
    "            total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        scheduler_config = dict(scheduler=scheduler, interval=\"step\")\n",
    "        return dict(optimizer=optimizer, lr_scheduler=scheduler_config)\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        chkpt = pl.callbacks.ModelCheckpoint(monitor=\"valid_auroc\", mode=\"max\")\n",
    "        return [chkpt]\n",
    "\n",
    "    def make_dataset(self, split):\n",
    "        X, y = datasets[split]\n",
    "        X, y = torch.Tensor(X), torch.Tensor(y)\n",
    "        return torch.utils.data.TensorDataset(X, y)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = self.make_dataset(\"train\")\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = self.make_dataset(\"valid\")\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size * 4,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375df7c7-605d-45c7-9b44-c0d5987a6b80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now let's fit the model to our pre-generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b5107-45cf-4aa6-a9a8-939a6a37e05f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = VanillaDetectionModel(batch_size=1024, learning_rate=0.02)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    precision=\"16-mixed\",\n",
    "    log_every_n_steps=5,\n",
    "    logger=pl.loggers.CSVLogger(\"logs\", name=\"vanilla-expt\"),\n",
    "    callbacks=[pl.callbacks.RichProgressBar()]\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7065e0-bafd-4c00-92f2-65435c0f3129",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Loss curve shows clear signs of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedccb47-b1c8-49e2-8cd9-736f046209cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotting.plot_run(\"vanilla-expt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19fc27e-88d3-49b1-b229-1282527aa829",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Keep the best model weights for inference later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f348d-c02f-4fce-bd36-55b84253561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_vanilla_weights = trainer.checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fcf82e-8309-4300-84e4-3bbf4b13cfff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Mission accomplished?\n",
    "- So it looks like we perform... well? Who knows, we'll address inference/evaluation later\n",
    "- Consider all the ways we threw out data/priors/physics to make this work\n",
    "    - Didn't use even close to all our background\n",
    "    - Even worse when you consider we can _shift_ IFOs wrt one another\n",
    "    - Only got to observe waveforms from one sky location/distance\n",
    "    - Only got to observe waveforms inserted in one particular noise background\n",
    "- We could just generate a _larger_ dataset, but just kicks the can"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ff486-24e0-4b18-b9ef-670bbeb9ec73",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"images/sample-flow.png\" width=50% style=\"display: block;margin-left:auto;margin-right:auto\"></img>\n",
    "What if we did this _in real time_ during training?\n",
    "- Take advantage of our data and physics to build more robust models\n",
    "    - Our data generation throughput was ~40 samples/s\n",
    "    - Our NN throughput was ~3500 samples/s\n",
    "    - Even if we get a lot faster, existing tools insufficient for real-time use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1936033-47cc-477a-961e-d019c8f304d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Enter `ml4gw`\n",
    "Library of `torch` utilities for common GW tasks/transforms\n",
    "- Align with existing APIs\n",
    "- `pip` installable\n",
    "- GPU accelerated, tensor-ized operations ensure efficient utilization\n",
    "- Auto-differentiation means we can take gradients through ops - build physics into models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33bfd0-f5d1-4fb0-aa3c-d548ccb0a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml4gw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e9794-c6d8-496e-9f3b-8f59a5ceef8f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Enter `ml4gw`\n",
    "Let's re-implement our sample-generation code using `ml4gw` dataloaders and transforms\n",
    "\n",
    "Start by clearing out the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a689c-40ea-4eab-8f5c-58c880118b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb857fef-c485-435a-86b0-9a69c1a7e669",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Training with `ml4gw`\n",
    "\n",
    "Start by defining a new model that will generate samples in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb12e1fe-1981-455e-81bb-94a55c25878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4gw import distributions, gw, transforms\n",
    "from ml4gw.dataloading import ChunkedTimeSeriesDataset, Hdf5TimeSeriesDataset\n",
    "from ml4gw.utils.slicing import sample_kernels\n",
    "\n",
    "class Ml4gwDetectionModel(VanillaDetectionModel):\n",
    "    \"\"\"\n",
    "    Model with additional methods for performing our\n",
    "    preprocessing augmentations in real-time on the GPU.\n",
    "    Also loads training background in chunks from disk,\n",
    "    then samples batches from chunks.\n",
    "\n",
    "    Note that the training and validation steps themselves\n",
    "    don't need to change at all: all we're doing is building\n",
    "    better ways of getting data to _feed_ to the training\n",
    "    step.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        ifos: list[str],\n",
    "        kernel_length: float,\n",
    "        fduration: float,\n",
    "        psd_length: float,\n",
    "        sample_rate: float,\n",
    "        fftlength: float,\n",
    "        chunk_length: float = 128,  # we'll talk about chunks in a second\n",
    "        reads_per_chunk: int = 40,\n",
    "        highpass: float = 32,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # real-time transformations defined at torch Modules\n",
    "        self.spectral_density = transforms.SpectralDensity(\n",
    "            sample_rate, fftlength, average=\"median\", fast=True\n",
    "        )\n",
    "        self.whitener = transforms.Whiten(fduration, sample_rate, highpass=highpass)\n",
    "\n",
    "        # get some geometry information about\n",
    "        # the interferometers we're going to project to\n",
    "        detector_tensors, vertices = gw.get_ifo_geometry(*ifos)\n",
    "        self.register_buffer(\"detector_tensors\", detector_tensors)\n",
    "        self.register_buffer(\"detector_vertices\", vertices)\n",
    "\n",
    "        # define some sky parameter distributions\n",
    "        self.declination = distributions.Cosine()\n",
    "        self.polarization = distributions.Uniform(0, torch.pi)\n",
    "        self.phi = distributions.Uniform(-torch.pi, torch.pi)  # relative RAs of detector and source\n",
    "\n",
    "        # rather than sample distances, we'll sample target SNRs.\n",
    "        # This way we can ensure we train our network on\n",
    "        # signals that are actually detectable. We'll use a distribution\n",
    "        # that looks roughly like our sampled SNR distribution\n",
    "        self.snr = distributions.PowerLaw(4, 100, 3)\n",
    "\n",
    "        # up front let's define some properties in units of samples\n",
    "        self.kernel_size = int(kernel_length * sample_rate)\n",
    "        self.window_size = self.kernel_size + int(fduration * sample_rate)\n",
    "        self.psd_size = int(psd_length * sample_rate)\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # lightning automatically calls this method before training starts.\n",
    "        # We'll use it to load in all our signals up front, though we could\n",
    "        # in principle sample these from disk for larger datasets\n",
    "        with h5py.File(\"data/signals.hdf5\", \"r\") as f:\n",
    "            group = f[\"train\"][\"polarizations\"]\n",
    "            self.Hp = torch.Tensor(group[\"plus\"][:])\n",
    "            self.Hc = torch.Tensor(group[\"cross\"][:])\n",
    "\n",
    "    def sample_waveforms(self, batch_size: int) -> tuple[torch.Tensor, ...]:\n",
    "        rvs = torch.rand(size=(batch_size,))\n",
    "        mask = rvs > 0.5\n",
    "        num_injections = mask.sum().item()\n",
    "\n",
    "        idx = torch.randint(len(self.Hp), size=(num_injections,))\n",
    "        hp = self.Hp[idx]\n",
    "        hc = self.Hc[idx]\n",
    "        return hc, hp, mask\n",
    "\n",
    "    def project_waveforms(self, hc: torch.Tensor, hp: torch.Tensor) -> torch.Tensor:\n",
    "        # sample sky parameters\n",
    "        N = len(hc)\n",
    "        declination = self.declination(N).to(hc)\n",
    "        polarization = self.polarization(N).to(hc)\n",
    "        phi = self.phi(N).to(hc)\n",
    "\n",
    "        # project to interferometer response\n",
    "        return gw.compute_observed_strain(\n",
    "            declination,\n",
    "            polarization,\n",
    "            phi,\n",
    "            detector_tensors=self.detector_tensors,\n",
    "            detector_vertices=self.detector_vertices,\n",
    "            sample_rate=self.hparams.sample_rate,\n",
    "            cross=hc,\n",
    "            plus=hp\n",
    "        )\n",
    "\n",
    "    def rescale_snrs(self, responses: torch.Tensor, psd: torch.Tensor) -> torch.Tensor:\n",
    "        # make sure everything has the same number of frequency bins\n",
    "        num_freqs = int(responses.size(-1) // 2) + 1\n",
    "        if psd.size(-1) != num_freqs:\n",
    "            psd = torch.nn.functional.interpolate(psd, size=(num_freqs,), mode=\"linear\")\n",
    "        snrs = gw.compute_network_snr(\n",
    "            responses.double(), psd, self.hparams.sample_rate, self.hparams.highpass\n",
    "        )\n",
    "\n",
    "        N = len(responses)\n",
    "        target_snrs = self.snr(N).to(snrs.device)\n",
    "        weights = target_snrs / snrs\n",
    "        return responses * weights.view(-1, 1, 1)\n",
    "\n",
    "    def sample_kernels(self, responses: torch.Tensor) -> torch.Tensor:\n",
    "        # slice off random views of each waveformto inject in arbitrary positions\n",
    "        responses = responses[:, :, -self.window_size:]\n",
    "\n",
    "        # pad so that at least half the kernel always contains signals\n",
    "        pad = [0, int(self.window_size // 2)]\n",
    "        responses = torch.nn.functional.pad(responses, pad)\n",
    "        return sample_kernels(responses, self.window_size, coincident=True)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def augment(self, X: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # break off \"background\" from target kernel and compute its PSD\n",
    "        # (in double precision since our scale is so small)\n",
    "        background, X = torch.split(X, [self.psd_size, self.window_size], dim=-1)\n",
    "        psd = self.spectral_density(background.double())\n",
    "\n",
    "        # sample at most batch_size signals from our bank and move them to our\n",
    "        # current device. Keep a mask that indicates which rows to inject in\n",
    "        batch_size = X.size(0)\n",
    "        hc, hp, mask = self.sample_waveforms(batch_size)\n",
    "        hc, hp, mask = hc.to(X), hp.to(X), mask.to(X.device)\n",
    "\n",
    "        # sample sky parameters and project to responses, then\n",
    "        # rescale the response according to a randomly sampled SNR\n",
    "        responses = self.project_waveforms(hc, hp)\n",
    "        responses = self.rescale_snrs(responses, psd[mask])\n",
    "\n",
    "        # randomly slice out a window of the waveform, add it\n",
    "        # to our background, then whiten everything\n",
    "        responses = self.sample_kernels(responses)\n",
    "        X[mask] += responses.float()\n",
    "        X = self.whitener(X, psd)\n",
    "\n",
    "        # create labels, marking 1s where we injected\n",
    "        y = torch.zeros((batch_size, 1), device=X.device)\n",
    "        y[mask] = 1\n",
    "        return X, y\n",
    "\n",
    "    def on_after_batch_transfer(self, batch, _):\n",
    "        # this is a parent method that lightning calls\n",
    "        # between when the batch gets moved to GPU and\n",
    "        # when it gets passed to the training_step.\n",
    "        # Apply our augmentations here\n",
    "        if self.trainer.training:\n",
    "            batch = self.augment(batch)\n",
    "        return batch\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # set up our dataloader so that the network \"sees\"\n",
    "        # twice as many samples as the number of waveforms\n",
    "        # during each epoch, so that on average it's going\n",
    "        # through the training waveforms once in each epoch\n",
    "        # (we sample with replacement, so it's not perfect).\n",
    "        samples_per_epoch = 2 * len(self.Hp)\n",
    "        batches_per_epoch = int((samples_per_epoch - 1) // self.hparams.batch_size) + 1\n",
    "        batches_per_chunk = int(batches_per_epoch // 10)\n",
    "        chunks_per_epoch = int(batches_per_epoch // batches_per_chunk) + 1\n",
    "\n",
    "        # Hdf5TimeSeries dataset samples batches from disk.\n",
    "        # In this instance, we'll make our batches really large so that\n",
    "        # we can treat them as chunks to sample training batches from\n",
    "        dataset = Hdf5TimeSeriesDataset(\n",
    "            \"data/background.hdf5\",\n",
    "            channels=self.hparams.ifos,\n",
    "            kernel_size=int(self.hparams.chunk_length * self.hparams.sample_rate),\n",
    "            batch_size=self.hparams.reads_per_chunk,\n",
    "            batches_per_epoch=chunks_per_epoch,\n",
    "            coincident=False,\n",
    "            path=\"train\"\n",
    "        )\n",
    "\n",
    "        # multiprocess this so there's always a new chunk ready when we need it\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "\n",
    "        # sample batches to pass to our NN from the chunks loaded from disk\n",
    "        return ChunkedTimeSeriesDataset(\n",
    "            dataloader,\n",
    "            kernel_size=self.window_size + self.psd_size,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            batches_per_chunk=batches_per_chunk,\n",
    "            coincident=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbdb9f8-c80a-43ae-9e56-91174862852d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now instantiate the model with all our preprocessing parameters from before.\n",
    "\n",
    "For dataloading, each chunk will read 20 random 128 second segments from our data on disk, from which we'll sample ~10% of batches in epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada555a-9be5-4539-ba6e-27bb28270104",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Ml4gwDetectionModel(\n",
    "    ifos,\n",
    "    kernel_length=1,\n",
    "    fduration=1,\n",
    "    psd_length=8,\n",
    "    sample_rate=sample_rate,\n",
    "    fftlength=2,\n",
    "    chunk_length=128,\n",
    "    reads_per_chunk=20,\n",
    "    highpass=32,\n",
    "    learning_rate=0.005,\n",
    "    batch_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2085e-5de1-48d6-a770-87baebde04af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now let's fit this model, and see if we can do any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312b21b-fd35-446d-8221-7d3e1e3edc5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = pl.loggers.CSVLogger(\"logs\", name=\"ml4gw-expt\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    precision=\"16-mixed\",\n",
    "    log_every_n_steps=5,\n",
    "    logger=logger,\n",
    "    callbacks=[pl.callbacks.RichProgressBar()]\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4555f9e9-92ef-4ddc-9643-ed3d26be1740",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Much more robust loss curves, looks like still headroom to grow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b186e-235c-45c7-a890-858ea7a3f69c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotting.plot_run(\"ml4gw-expt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267b5c66-0da1-4a3f-81f5-e88b2263e0ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Save our best weights again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f320e-2823-4fde-9a7f-bfe55cb2f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ml4gw_weights = trainer.checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f92517a-0512-4490-bef4-6630966b0d2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Inference and Evaluation\n",
    "- So we've gone through the effort to train not just one but _two_ models\n",
    "- Validation scores are nice, but to deploy them we'll need meaningful metrics on a _lot_ of data\n",
    "- Key questions:\n",
    "    - At a given NN output threshold, how many events per unit time do I expect to detect?\n",
    "    - At the same threshold, how many false alarms per unit time do I expect to raise?\n",
    "- Measure **sensitive volume** vs. false alarm rate using 2 weeks of time-shifted real background data\n",
    "    - Mostly care about tail false alarm events, so quality of estimate heavily reliant on volume of data\n",
    "    - In principle would want at least $\\mathcal{O}$(years)\n",
    "- For full details, see e.g. Tiwari 2017: https://arxiv.org/abs/1712.00482"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e807e-ce90-48b9-89a8-d85c217b0a80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Streaming inference\n",
    "- High-frequency inference means NN inputs will contain redundant data\n",
    "- Adopt a stateful streaming strategy to minimize I/O at the expense of introducing serial processing\n",
    "- `ml4gw` contains some useful common stateful steps\n",
    "<img src=\"images/inference-strategy.png\" width=50% style=\"display: block;margin-left:auto;margin-right:auto\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff46ba2-66f8-4926-b8c7-b7c47029ac1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Local inference\n",
    "- We'll start with the simplest approach: use our model and `ml4gw` preprocessing steps locally, in-memory\n",
    "- Build a local stateful model we can use. See code for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7be744-cd37-4054-95a4-5e32823707e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import infer\n",
    "\n",
    "batcher = infer.BatchGenerator(\n",
    "    model.spectral_density,\n",
    "    model.whitener,\n",
    "    len(ifos),\n",
    "    kernel_length=1,\n",
    "    fduration=1,\n",
    "    psd_length=64,\n",
    "    inference_sampling_rate=8,\n",
    "    sample_rate=sample_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485a1da-d9c8-4885-801e-dbfb3e23f1b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Won't go into details of how we turn NN predictions into GW \"events\", see upcoming paper. Just need to define a function for generating that timeseries of NN predictions locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5642257b-d4a4-4946-addb-d6d6814fbffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalInferenceFn:\n",
    "    def __init__(self, model, chkpt_path, device=\"cuda\") -> None:\n",
    "        # load in our model checkpoint and set the nn\n",
    "        # weights to the appropriate values\n",
    "        checkpoint = torch.load(chkpt_path)\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "        state_dict = {k.strip(\"n.\"): v for k, v in state_dict.items() if k.startswith(\"nn.\")}\n",
    "        model.nn.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, streaming_iterator, pbar):\n",
    "        bg_preds, fg_preds = [], []\n",
    "        state = batcher.get_initial_state().to(self.device)\n",
    "        for X in streaming_iterator:\n",
    "            # move data onto GPU. X contains alternating\n",
    "            # background and signal examples\n",
    "            X = torch.Tensor(X).to(self.device)\n",
    "\n",
    "            # now fan it out into a batch of overlapping samples\n",
    "            # and retrieve the updated state\n",
    "            batch, state = batcher(X, state)\n",
    "\n",
    "            # do inference and separate out background and signal\n",
    "            preds = self.model(batch)[:, 0]\n",
    "            bg_preds.append(preds[::2])\n",
    "            fg_preds.append(preds[1::2])\n",
    "\n",
    "            # support a progress bar to keep track of everything\n",
    "            pbar.update(1)\n",
    "\n",
    "        # concatenate all our predictions into a timeseries of\n",
    "        # NN outputs through which we'll comb for events\n",
    "        bg_preds = torch.cat(bg_preds).cpu().numpy()\n",
    "        fg_preds = torch.cat(fg_preds).cpu().numpy()\n",
    "        return bg_preds, fg_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b593b-737f-45c0-aec4-3bae4a3b303c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now let's run inference for both our models and see\n",
    "1. How long it takes\n",
    "2. How they compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbd310-43e5-4649-b5dd-7730a8b61a60",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "infer_params = dict(\n",
    "    ifos=ifos,\n",
    "    kernel_length=1,  # length of input windows to network\n",
    "    psd_length=64,  # how long of a segment to use for PSD estimation\n",
    "    fduration=1,  # filter settle-in length\n",
    "    inference_sampling_rate=8,  # how frequently to sample input windows\n",
    "    batch_size=4096,\n",
    "    pool_length=8  # how we keep from double counting events\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f4696a-1ec1-4fb9-9fdf-edec69fbadaa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vanilla_inference_fn = LocalInferenceFn(model, best_vanilla_weights)\n",
    "vanilla_results = infer.infer(vanilla_inference_fn, **infer_params)\n",
    "\n",
    "ml4gw_inference_fn = LocalInferenceFn(model, best_ml4gw_weights)\n",
    "ml4gw_results = infer.infer(ml4gw_inference_fn, **infer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e9b977-cc71-4186-afa2-e09c225715b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "How do these two models stack up against one another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b090014-a8b9-42ce-8b26-9f5005895711",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_evaluation(vanilla=vanilla_results, ml4gw=ml4gw_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3531ef-b81b-4d53-9780-c0014c45a35a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Good to see the model trained with better physics perform better!\n",
    "- But it took nearly an hour to analyze 2 weeks worth of data\n",
    "- Need to scale up analyze useful lengths of time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7a332-71bc-4ffd-9ddc-4c49a01516f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Inference-as-a-Service\n",
    "`lightning` has good distribution APIs, but some other things to consider\n",
    "- Integration with other models in other frameworks (e.g. TensorFlow)\n",
    "- Use of accelerated frameworks like TensorRT\n",
    "- One model insufficient to saturate GPU compute - use multiple concurrent execution instances\n",
    "- One data stream insufficient to saturate GPU compute - need multiple streams per GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe1d9e7-a0d9-4426-9f3b-fadd5aff8f2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Inference-as-a-Service\n",
    "Central application responsible for executing model inference\n",
    "- Accepts requests via lightweight client APIs\n",
    "    - Abstracts implementation and framework details\n",
    "- Asynchronously chedules requests from different clients\n",
    "- Intra-device parallel execution ensures accelerators are saturated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae849eb-49c0-4d76-a292-6aab289c4f16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Inference-as-a-Service - Triton Inference Server\n",
    "NVIDIA's [Triton Inference Server](https://developer.nvidia.com/triton-inference-server) gets good performance out-of-the-box\n",
    "- Stateful support can be tricky to navigate\n",
    "- Configurations built around non-Pythonic protobufs\n",
    "- No integration with frameworks to export/convert between formats easily\n",
    "- Lots of boilerplate - most info should be built into model\n",
    "    - Data types, shapes, tensor names, etc.\n",
    "    - Recently added more support for inferring properties, but still lots of use cases where need to touch config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba4092-be25-4896-ab3c-683d40a85e7d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Enter `hermes`\n",
    "Set of utilities for simplifying streaming inference-as-a-service deployment\n",
    "- Not currently `pip` installable, but undergoing overhaul\n",
    "- Keep an eye on repo for release in coming week\n",
    "\n",
    "Let's\n",
    "1. Export our preprocessor and NN using `hermes`\n",
    "2. serve up this **ensemble** using Triton on 2 GPUs\n",
    "3. See how much better our throughput gets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9460f4d-80f2-4e0d-81b0-745559309163",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hermes.quiver import ModelRepository, Platform\n",
    "\n",
    "# create a fresh model repository and add a new entry for our nn to it.\n",
    "# We'll export the model to an accelerated TensorRT executable in FP16.\n",
    "# Note that our batch size is double: one for background and one for signals\n",
    "repo = ModelRepository(\"repo\", clean=True)\n",
    "nn = repo.add(\"detector\", platform=Platform.TENSORRT)\n",
    "export_path = nn.export_version(\n",
    "    model.nn.to(\"cpu\"),\n",
    "    input_shapes={\"strain\": (2 * infer_params[\"batch_size\"], 2, sample_rate)},\n",
    "    output_names=[\"detection_statistic\"],\n",
    "    use_fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa07b3-a906-45db-90bc-359a64c88218",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "So what's in our model repository now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67bf3d-4a7d-486b-9bf8-7b5d36b12c8f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ls repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7220ed-0bce-4af8-9759-491ae3ac9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls repo/detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b9950-a516-4025-850f-f4c4ebf9e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls repo/detector/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a0021-e906-4a8a-b4cc-e545456a6091",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "For our preprocessor, we'll need to use some utils to export it as a stateful model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf4a49-0a67-48f4-85df-61e0d894bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hermes.quiver.streaming.utils import add_streaming_model\n",
    "\n",
    "update_size = batcher.step_size * infer_params[\"batch_size\"]\n",
    "preprocessor = add_streaming_model(\n",
    "    repo,\n",
    "    streaming_layer=batcher.to(\"cpu\"),\n",
    "    name=\"preprocessor\",\n",
    "    input_shape=(2, len(ifos), update_size),\n",
    "    state_shapes=[(2, len(ifos), batcher.state_size)],\n",
    "    platform=Platform.TORCHSCRIPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14370dd6-d104-4b19-aa05-d83da2ab9b47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ls repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d14749-648b-479b-a2cb-8c385b18a26a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Finally, we'll plug all this together with an \"ensemble\" model: a meta model that represents a chain of execution of multiple sub-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9d47d-67d8-4e52-bac5-39a4188ffe0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ensemble = repo.add(\"streaming-detector\", platform=Platform.ENSEMBLE)\n",
    "ensemble.add_input(preprocessor.inputs[\"INPUT__0\"])\n",
    "ensemble.pipe(preprocessor.outputs[\"OUTPUT__0\"], nn.inputs[\"strain\"])\n",
    "ensemble.add_output(nn.outputs[\"detection_statistic\"])\n",
    "_ = ensemble.export_version(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d29c6-92e2-433d-9d12-45b12dcaee23",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ls repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b209b-0cea-4ef0-8a02-c68a11d5b3bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now go start a a Triton instance in a separate terminal. You'll need at least the version from July 2023 to support stateful PyTorch models:\n",
    "\n",
    "```console\n",
    "apptainer pull triton.sif docker://nvcr.io/nvidia/tritonserver:23.07-py3\n",
    "APPTAINERENV_CUDA_VISIBLE_DEVICES=0,1 apptainer run --nv triton.sif \\\n",
    "    /opt/tritonserver/bin/tritonserver --model-repository repo\n",
    "```\n",
    "\n",
    "`hermes` includes utilities for doing this locally, but didn't want to do container-in-container - more on this at end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ee7846-5758-46fd-8b3a-7f4862dc6744",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The key to IaaS with `hermes` is using callback functions to asynchronously handle the server's response (containing the NN predictions). Let's build a particularly simple one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69348fa-3489-4a39-9552-b67c793b437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def reset(self, num_steps, pbar):\n",
    "        self.pbar = pbar\n",
    "        self.num_steps = num_steps\n",
    "        self.background_preds = []\n",
    "        self.foreground_preds = []\n",
    "\n",
    "    def __call__(self, x, request_id, sequence_id):\n",
    "        self.background_preds.append(x[::2, 0])\n",
    "        self.foreground_preds.append(x[1::2, 0])\n",
    "        self.pbar.update(1)\n",
    "        if (request_id + 1) == self.num_steps:\n",
    "            background_preds = np.concatenate(self.background_preds)\n",
    "            foreground_preds = np.concatenate(self.foreground_preds)\n",
    "            return background_preds, foreground_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10cc80-5db0-4548-9674-8923eaf1da70",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Then all our inference function needs is a `hermes` `InferenceClient` instance which it will use to stream data to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472b1cc-95d3-4cd7-83ee-08fb955dbef3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class TritonInferenceFn:\n",
    "    def __init__(self, client, callback, rate):\n",
    "        self.client = client\n",
    "        self.callback = callback\n",
    "        self.rate = rate\n",
    "\n",
    "    def __call__(self, streaming_iterator, pbar):\n",
    "        num_steps = len(streaming_iterator)\n",
    "        self.callback.reset(num_steps, pbar)\n",
    "        for i, X in enumerate(streaming_iterator):\n",
    "            self.client.infer(\n",
    "                X.astype(np.float32),\n",
    "                request_id=i,\n",
    "                sequence_id=1001,\n",
    "                sequence_start=i == 0,\n",
    "                sequence_end=(i + 1) == num_steps\n",
    "            )\n",
    "            time.sleep(1 / self.rate)\n",
    "\n",
    "        while True:\n",
    "            response = self.client.get()\n",
    "            if response is not None:\n",
    "                return response\n",
    "            time.sleep(1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea3208-e3a6-454b-8639-e6af64b21e96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now we're all set to do as-a-service inference on our end-to-end streaming model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db47213b-60d1-4b71-b2f3-72ea416b9c6d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hermes.aeriel.client import InferenceClient\n",
    "\n",
    "infer.wait_for_model(\"streaming-detector\")\n",
    "callback = Callback()\n",
    "client = InferenceClient(\"localhost:8001\", \"streaming-detector\", callback=callback)\n",
    "\n",
    "# entering the context starts a streaming connection to\n",
    "# the server with a background thread to execute the callback\n",
    "with client:\n",
    "    triton_infer_fn = TritonInferenceFn(client, callback, rate=6)\n",
    "    triton_results = infer.infer(triton_infer_fn, **infer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ccbd2-cedd-40e4-8758-db29ca388168",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Added a second GPU, but went over 3x faster!\n",
    "- Benefits from accelerated frameworks/mixed precision inference\n",
    "- In IaaS model, scale come for free - easy to add GPUs/nodes\n",
    "- Limited on requests side - need infra layer for distributing clients e.g. `ray`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904bb0f-a8f5-4499-bd7f-3599e780c63f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Sure it's faster, but let's also make sure that the served up model achieves comparable performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012e45b3-6a7e-4b26-a8da-e62aff3e3a36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotting.plot_evaluation(\n",
    "    vanilla=vanilla_results,\n",
    "    ml4gw=ml4gw_results,\n",
    "    ml4gw_on_triton=triton_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e595771d-fcd8-4b96-9918-8ce0be17669d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Conclusions and Next Steps\n",
    "- `ml4gw` and `hermes` represent the seeds of an ecosystem to do better GW physics with ML\n",
    "- Lots to do\n",
    "    - More use cases $\\rightarrow$ more and better features\n",
    "    - Additional abstractions for increased scale\n",
    "    - Always looking for collaborators - let's chat!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b22be53-1c20-48a5-8e39-305d4c46c18f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Thank you!\n",
    "\n",
    "Questions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "rise": {
   "height": "675",
   "scroll": true,
   "width": "1200"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
