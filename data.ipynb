{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33cc425b-27c0-442e-9d99-e8786a9978ae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/env/lib/python3.9/site-packages/gwpy/time/__init__.py:36: UserWarning: Wswiglal-redir-stdio:\n",
      "\n",
      "SWIGLAL standard output/error redirection is enabled in IPython.\n",
      "This may lead to performance penalties. To disable locally, use:\n",
      "\n",
      "with lal.no_swig_redirect_standard_output_error():\n",
      "    ...\n",
      "\n",
      "To disable globally, use:\n",
      "\n",
      "lal.swig_redirect_standard_output_error(True)\n",
      "\n",
      "Note however that this will likely lead to error messages from\n",
      "LAL functions being either misdirected or lost when called from\n",
      "Jupyter notebooks.\n",
      "\n",
      "To suppress this warning, use:\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")\n",
      "import lal\n",
      "\n",
      "  from lal import LIGOTimeGPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 active segments between 01 April 2019 and 22 April 2019 corresponding to 7.06 days worth of data\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from gwpy.time import from_gps, to_gps\n",
    "from gwpy.timeseries import TimeSeriesDict\n",
    "from gwpy.segments import DataQualityDict, DataQualityFlag, SegmentList\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# set up some data query hyperparameters up front\n",
    "IFOS = [\"H1\", \"L1\"]\n",
    "START = \"01 April 2019\"\n",
    "NUM_TRAIN_WEEKS = 1\n",
    "NUM_TEST_WEEKS = 2\n",
    "MIN_DURATION = 8192\n",
    "\n",
    "start_date = datetime.strptime(START, \"%d %B %Y\")\n",
    "duration = timedelta(weeks=NUM_TRAIN_WEEKS + NUM_TEST_WEEKS)\n",
    "end_date = start_date + duration\n",
    "\n",
    "# convert to GPS timestamps\n",
    "start = to_gps(start_date).gpsSeconds\n",
    "end = to_gps(end_date).gpsSeconds\n",
    "\n",
    "# query segments where all interferometers have\n",
    "# open, analysis-ready data\n",
    "segments = DataQualityDict()\n",
    "for ifo in IFOS:\n",
    "    dqf = f\"{ifo}_DATA\"\n",
    "    segments[ifo] = DataQualityFlag.fetch_open_data(dqf, start, end)\n",
    "segments = segments.intersection().active\n",
    "\n",
    "# filter out any segments that are too short\n",
    "# to be worth analyzing\n",
    "if MIN_DURATION is not None:\n",
    "    segments = filter(lambda i: i[1] - i[0] >= MIN_DURATION, segments)\n",
    "    segments = SegmentList(segments)\n",
    "\n",
    "# summarize our findings for the good people\n",
    "total_duration = sum([i[1] - i[0] for i in segments])\n",
    "print(\n",
    "    \"{} active segments between {} and {} \"\n",
    "    \"corresponding to {:0.2f} days worth of data\".format(\n",
    "        len(segments),\n",
    "        start_date.strftime(\"%d %B %Y\"),\n",
    "        end_date.strftime(\"%d %B %Y\"),\n",
    "        total_duration / 3600 / 24\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07cf574a-59fe-4617-95db-ff40e401e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_test(segment):\n",
    "    dt = timedelta(weeks=NUM_TRAIN_WEEKS)\n",
    "    return (from_gps(segment[0]) - dt) > start_date\n",
    "\n",
    "\n",
    "def is_train(segment):\n",
    "    return not is_test(segment)\n",
    "\n",
    "\n",
    "train_segments = list(filter(is_train, segments))\n",
    "test_segments = list(filter(is_test, segments))\n",
    "\n",
    "valid_frac = 0.2\n",
    "durations = [i[1] - i[0] for i in train_segments]\n",
    "durations = np.cumsum(durations)\n",
    "idx = np.searchsorted(durations, (1 - valid_frac) * durations[-1])\n",
    "valid_segments = train_segments[idx:]\n",
    "train_segments = train_segments[:idx]\n",
    "\n",
    "background_splits = dict(\n",
    "    train=train_segments,\n",
    "    valid=valid_segments,\n",
    "    test=test_segments\n",
    ")\n",
    "split_map = {i: k for k, v in background_splits.items() for i in v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4c769a4-03c8-4674-9cef-e3da9d10eff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 33/33 [01:07<00:00,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "psds = {i: 0 for i in IFOS}\n",
    "with h5py.File(\"data/background.hdf5\", mode=\"a\") as f:\n",
    "    for segment in tqdm(segments):\n",
    "        start, end = segment\n",
    "        split = split_map[segment]\n",
    "\n",
    "        try:\n",
    "            group = f[split]\n",
    "        except KeyError:\n",
    "            group = f.create_group(split)\n",
    "    \n",
    "        duration = end - start\n",
    "        name = f\"{start}-{duration}\"\n",
    "        try:\n",
    "            dataset = group[name]\n",
    "        except KeyError:\n",
    "            dataset = group.create_group(name)\n",
    "            dataset.attrs[\"start\"] = start\n",
    "            dataset.attrs[\"duration\"] = duration\n",
    "            x = TimeSeriesDict.get(\n",
    "                channels=[i + \":GDS-CALIB_STRAIN\" for i in IFOS],\n",
    "                start=start,\n",
    "                end=end\n",
    "            )\n",
    "            x = x.resample(2048)\n",
    "            x = {i: x.pop(f\"{i}:GDS-CALIB_STRAIN\") for i in IFOS}\n",
    "            x = TimeSeriesDict(x)\n",
    "            x.write(dataset, chunks=(2048 * 8,))\n",
    "        else:\n",
    "            if split == \"train\":\n",
    "                x = TimeSeriesDict.read(dataset, path=IFOS)\n",
    "\n",
    "        if split == \"train\":\n",
    "            for ifo in IFOS:\n",
    "                psd = x[ifo].psd(fftlength=2, method=\"median\", window=\"hann\")\n",
    "                psds[ifo] += psd / len(train_segments)\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9ff873b-78d6-4dd6-9886-e7c17fc01a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.cosmology import Planck15\n",
    "from bilby.core.prior import (\n",
    "    ConditionalPowerLaw,\n",
    "    ConditionalPriorDict,\n",
    "    Constraint,\n",
    "    Cosine,\n",
    "    Gaussian,\n",
    "    PowerLaw,\n",
    "    PriorDict,\n",
    "    Sine,\n",
    "    Uniform\n",
    ")\n",
    "from bilby.gw.prior import UniformComovingVolume\n",
    "\n",
    "def mass_condition(_, mass_1):\n",
    "    return dict(maximum=mass_1)\n",
    "\n",
    "msun = r\"$M_{\\odot}$\"\n",
    "prior = dict(\n",
    "    redshift=UniformComovingVolume(0, 2, name=\"redshift\", cosmology=Planck15),\n",
    "    mass1=PowerLaw(alpha=-2.35, minimum=5, maximum=100, unit=msun),\n",
    "    mass2=ConditionalPowerLaw(\n",
    "        condition_func=mass_condition,\n",
    "        alpha=1,\n",
    "        minimum=5,\n",
    "        maximum=100,\n",
    "        unit=msun,\n",
    "    ),\n",
    "    spin1_mag=Uniform(0, 0.998),\n",
    "    spin2_mag=Uniform(0, 0.998),\n",
    "    spin1_tilt=Sine(unit=\"rad\"),\n",
    "    spin2_tilt=Sine(unit=\"rad\"),\n",
    "    spin1_azimuth=Uniform(0, 2 * np.pi),\n",
    "    spin2_azimuth=Uniform(0, 2 * np.pi),\n",
    "    inclination=Sine(unit=\"rad\")\n",
    ")\n",
    "prior = PriorDict(prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d81964-9c41-4316-860f-74251c1cd85e",
   "metadata": {},
   "source": [
    "Now in the spirit of absolute clarity, I could very explicitly write out the code for the various steps involved in generating a waveform dataset.\n",
    "1. Sample intrinsic parameters of a source event\n",
    "2. Simulate the waveform of gravitational strain produced by it\n",
    "3. Sample sky parameters relative to the geocenter\n",
    "4. Use these to compute the detector's response to the waveform\n",
    "5. Measure the SNR of this response with respect to the PSD of the training data\n",
    "6. If the SNR is over our threshold, keep the waveform. Otherwise, discard it and record a rejection\n",
    "7. Repeat until the threshold is passed\n",
    "8. Repeat until you have enough waveforms to fit a big neural network\n",
    "\n",
    "However, it turns out this process is pretty laborious, and making it run in an amount of time that I would have any intention of sitting around for takes some slightly fancy footwork.\n",
    "It's nothing too crazy, but I'll resort to building some custom objects which lend themselves to multiprocessing more naturally.\n",
    "\n",
    "In practice, we normally end up massively parallelizing this over a compute cluster using HTCondor, but doing this with any level of elegance requires quite a bit of extra infra that goes outside the scope of this tutorial.\n",
    "Alternatively, we could use some of our `ml4gw` tools to accelerate the detector response and SNR calculations and even move them to GPU.\n",
    "However, that would defeat the secondary purpose of illustrating what this workflow looked like before these tools.\n",
    "Moreover, the main bottleneck here is on the waveform simulation, so we don't gain too much by accelerating/tensor-fying these other operations.\n",
    "\n",
    "That said, `pycbc` has `pycuda` bindings which can move the waveform simulation to GPU as well, which seems to speed that up by about a factor of 3.\n",
    "However, in the realisitic distributed computing case, our clusters tend to have many more CPUs than GPUs available, and so while moving this compute to GPU brings improvements in per-waveform _latency_, it ultimately restricts total _throughput_ by restricting the amount of computing resources you're able to parallelize over.\n",
    "Of course, this can also be solved by more robust infrastructure that uses both CPUs and GPUs and dynamically picks the appropriate compute backend, but now we're going out-of-scope again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a7a3018-fdef-4066-ad82-f3f3754141f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycbc.detector import Detector\n",
    "from pycbc.filter.matchedfilter import sigmasq, make_frequency_series\n",
    "from pycbc.types import FrequencySeries, TimeSeries\n",
    "from pycbc.waveform import get_td_waveform\n",
    "\n",
    "\n",
    "class WaveformSampler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        prior: PriorDict,\n",
    "        max_length: float,\n",
    "        sample_rate: float,\n",
    "        f_lower: float = 10,\n",
    "        f_ref: float = 50,\n",
    "        approximant: str = \"IMRPhenomD\",\n",
    "        snr_threshold: float = 0,\n",
    "        highpass: float = 0,\n",
    "        **psds: np.ndarray\n",
    "    ) -> None:\n",
    "        self.prior = prior\n",
    "        self.psds = psds\n",
    "        self.max_size = int(max_length * sample_rate)\n",
    "        self.delta_t = 1 / sample_rate\n",
    "        self.f_lower = f_lower\n",
    "        self.f_ref = f_ref\n",
    "        self.approximant = approximant\n",
    "\n",
    "        self.highpass = highpass\n",
    "        self.snr_threshold = snr_threshold\n",
    "        self.skyloc_prior = PriorDict(dict(\n",
    "            declination=Cosine(),\n",
    "            right_ascension=Uniform(0, 2 * np.pi),\n",
    "            polarization=Uniform(0, np.pi)\n",
    "        ))\n",
    "\n",
    "    def generate_waveform(self, **params):\n",
    "        hp, hc = get_td_waveform(\n",
    "            **params,\n",
    "            delta_t=self.delta_t,\n",
    "            f_lower=self.f_lower,\n",
    "            f_ref=self.f_ref,\n",
    "            approximant=self.approximant\n",
    "        )\n",
    "\n",
    "        # IMRPhenomD includes a bunch of extra time\n",
    "        # after the ringdown that we don't want\n",
    "        mask = hp.sample_times.data < 0.05\n",
    "        hp, hc = hp[mask], hc[mask]\n",
    "        hp, hc =  hp[-self.max_size:], hc[-self.max_size:]\n",
    "\n",
    "        # pycbc waveforms are scaled up by a factor of\n",
    "        # 1 / delta_t for some reason I can't understand\n",
    "        hp *= self.delta_t\n",
    "        hc *= self.delta_t\n",
    "\n",
    "        # convert back to timeseries to take advantage\n",
    "        # of nice things like built-in fft methods\n",
    "        hp = TimeSeries(hp, delta_t=self.delta_t)\n",
    "        hc = TimeSeries(hc, delta_t=self.delta_t)\n",
    "        return hp, hc\n",
    "\n",
    "    def sample_params(self):\n",
    "        params = self.prior.sample()\n",
    "        spherical = [\"mag\", \"tilt\", \"azimuth\"]\n",
    "        for i in range(2):\n",
    "            mag, tilt, azi = [params.pop(f\"spin{i+1}_{j}\") for j in spherical]\n",
    "            params[f\"spin{i+1}_x\"] = mag * np.sin(tilt) * np.cos(azi)\n",
    "            params[f\"spin{i+1}_y\"] = mag * np.sin(tilt) * np.sin(azi)\n",
    "            params[f\"spin{i+1}_z\"] = mag * np.cos(tilt)\n",
    "        return params\n",
    "\n",
    "    def compute_ht(self, ifo, hc, hp, t_gps, skyloc):\n",
    "        detector = Detector(ifo)\n",
    "        fp, fc = detector.antenna_pattern(t_gps=t_gps, **skyloc)\n",
    "        return fp * hp + fc * hc\n",
    "\n",
    "    def compute_snr(self, ifo, ht):\n",
    "        htilde = make_frequency_series(ht)\n",
    "        psd = self.psds[ifo].interpolate(htilde.delta_f)\n",
    "        psd = FrequencySeries(psd.value, delta_f=htilde.delta_f)\n",
    "        return sigmasq(htilde, psd, low_frequency_cutoff=self.highpass)\n",
    "\n",
    "    def __call__(self, t_gps):\n",
    "        num_rejected = 0\n",
    "        while True:\n",
    "            params = self.sample_params()\n",
    "            hp, hc = self.generate_waveform(**params)\n",
    "            skyloc = self.skyloc_prior.sample()\n",
    "            if self.snr_threshold == 0 or not self.psds:\n",
    "                break\n",
    "\n",
    "            snr = 0\n",
    "            for ifo, psd in self.psds.items():\n",
    "                ht = self.compute_ht(ifo, hc, hp, t_gps, skyloc)\n",
    "                snr += self.compute_snr(ifo, ht)\n",
    "            snr **= 0.5\n",
    "            if snr >= self.snr_threshold:\n",
    "                break\n",
    "            num_rejected += 1\n",
    "\n",
    "        params |= skyloc\n",
    "        params[\"snr\"] = snr\n",
    "        return hp, hc, params, num_rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b88f02c-90ff-489a-bc61-43a9b339a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = WaveformSampler(\n",
    "    prior,\n",
    "    max_length=4,\n",
    "    sample_rate=2048,\n",
    "    f_lower=10,\n",
    "    f_ref=50,\n",
    "    approximant=\"IMRPhenomD\",\n",
    "    snr_threshold=4,\n",
    "    highpass=32,\n",
    "    **psds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f25ee33-0411-4dcd-86b7-fe27f679c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sizes = dict(train=50000, valid=10000)\n",
    "t_gps = to_gps(start_date).gpsSeconds\n",
    "\n",
    "f = h5py.File(\"data/signals.hdf5\", \"w\")\n",
    "ex = ProcessPoolExecutor(16)\n",
    "try:\n",
    "    # allocate memory for polarization datasets up front\n",
    "    # while our waveforms begin to generate\n",
    "    futures = {}\n",
    "    for split, size in split_sizes.items():\n",
    "        fs = [ex.submit(sampler, t_gps=t_gps) for _ in range(size)]\n",
    "        fs = enumerate(fs)\n",
    "        futures.update({future: (i, split) for i, future in fs})\n",
    "\n",
    "        group = f.create_group(split)\n",
    "        group.attrs[\"num_rejected\"] = 0\n",
    "\n",
    "        polars = group.create_group(\"polarizations\")\n",
    "        params = group.create_group(\"parameters\")\n",
    "\n",
    "        print(f\"Initializing datasets for {split} split\")\n",
    "        polars.create_dataset(\"cross\", shape=(size, sampler.max_size))\n",
    "        polars.create_dataset(\"plus\", shape=(size, sampler.max_size))\n",
    "        print(\"Waveform datasets initialized\")\n",
    "\n",
    "    # write them to disk as they come in\n",
    "    fs = list(futures.keys())\n",
    "    for future in tqdm(as_completed(fs), total=len(fs)):\n",
    "        idx, split = futures.pop(future)\n",
    "        hp, hc, p, n_rej = future.result()\n",
    "\n",
    "        group = f[split]\n",
    "        group[\"polarizations\"][\"plus\"][idx, -len(hp):] = hp\n",
    "        group[\"polarizations\"][\"cross\"][idx, -len(hc):] = hc\n",
    "\n",
    "        params = group[\"parameters\"]\n",
    "        for k, v in p.items():\n",
    "            try:\n",
    "                dset = params[k]\n",
    "            except KeyError:\n",
    "                size = split_sizes[split]\n",
    "                dset = params.create_dataset(k, shape=(size,))\n",
    "            dset[idx] = v\n",
    "        group.attrs[\"num_rejected\"] += n_rej\n",
    "        if not (idx + 1) % 1000:\n",
    "            f.flush()\n",
    "except Exception:\n",
    "    f.close()\n",
    "    ex.shutdown(wait=False, cancel_futures=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0ad699-ed7c-46fd-80ad-807f582e5cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "test_background_time_weeks = 2\n",
    "seconds_per_week = 3600 * 24 * 7\n",
    "test_background_time_seconds = test_background_time_weeks * seconds_per_week\n",
    "\n",
    "test_segments = background_splits[\"test\"]\n",
    "test_background_livetime = sum([i[1] - i[0] for i in test_segments])\n",
    "\n",
    "num_shifts = ceil(test_background_time_seconds / test_background_livetime)\n",
    "shifts_per_ifo = ceil(num_shifts / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de57afe-7816-43c0-a248-e1926740b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import windows\n",
    "\n",
    "\n",
    "class TestWaveformSampler(WaveformSampler):\n",
    "    def __init__(self, *args, taper_length=0.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        taper_size = int(taper_length / self.delta_t)\n",
    "        taper = windows.hann(2 * taper_size)\n",
    "        self.taper = taper[:taper_size]\n",
    "\n",
    "    \"\"\"\n",
    "    This time, a call to the sampler will generate\n",
    "    all the waveforms for a\n",
    "    \"\"\"\n",
    "    def __call__(self, t_gps: float):\n",
    "        num_rejected = 0\n",
    "        while True:\n",
    "            params = self.sample_params()\n",
    "            hp, hc = self.generate_waveform(**params)\n",
    "            skyloc = self.skyloc_prior.sample()\n",
    "\n",
    "            hts, snr = {}, 0\n",
    "            for ifo, psd in self.psds.items():\n",
    "                ht = self.compute_ht(ifo, hc, hp, t_gps, skyloc)\n",
    "                ht[:len(self.taper)] *= self.taper\n",
    "                hts[ifo] = ht\n",
    "                if self.snr_threshold > 0:\n",
    "                    snr += self.compute_snr(ifo, ht)\n",
    "            snr **= 0.5\n",
    "            if snr >= self.snr_threshold:\n",
    "                break\n",
    "            num_rejected += 1\n",
    "\n",
    "        params |= skyloc\n",
    "        params[\"snr\"] = snr\n",
    "\n",
    "        # compute the probability of this sample\n",
    "        # under _just_ the mass components of\n",
    "        # the prior, this way we can use importance\n",
    "        # sampling to reweight to other mass priors\n",
    "        # after we run inference\n",
    "        masses = {f\"mass{i+1}\": params[f\"mass{i+1}\"] for i in range(2)}\n",
    "        params[\"prob\"] = self.prior.prob(masses)\n",
    "        return hts, params, num_rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43b6f88b-be50-4f92-9bee-9fdad38bee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = TestWaveformSampler(\n",
    "    prior,\n",
    "    max_length=4,\n",
    "    sample_rate=2048,\n",
    "    f_lower=10,\n",
    "    f_ref=50,\n",
    "    approximant=\"IMRPhenomD\",\n",
    "    snr_threshold=4,\n",
    "    highpass=32,\n",
    "    **psds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9984ede7-5c00-46ce-9270-ddaf59927018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2281439/3697181178.py:43: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  params[\"prob\"] = self.prior.prob(masses)\n",
      "/tmp/ipykernel_2281439/3697181178.py:43: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  params[\"prob\"] = self.prior.prob(masses)\n",
      "/tmp/ipykernel_2281439/3697181178.py:43: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  params[\"prob\"] = self.prior.prob(masses)\n",
      "/tmp/ipykernel_2281439/3697181178.py:43: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  params[\"prob\"] = self.prior.prob(masses)\n",
      "/tmp/ipykernel_2281439/3697181178.py:43: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  params[\"prob\"] = self.prior.prob(masses)\n",
      "/tmp/ipykernel_2281439/3697181178.py:43: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  params[\"prob\"] = self.prior.prob(masses)\n",
      "/tmp/ipykernel_2281439/3697181178.py:43: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  params[\"prob\"] = self.prior.prob(masses)\n",
      "/tmp/ipykernel_2281439/3697181178.py:43: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  params[\"prob\"] = self.prior.prob(masses)\n",
      "/tmp/ipykernel_2281439/3697181178.py:43: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  params[\"prob\"] = self.prior.prob(masses)\n",
      "/tmp/ipykernel_2281439/3697181178.py:43: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  params[\"prob\"] = self.prior.prob(masses)\n",
      "/tmp/ipykernel_2281439/3697181178.py:43: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  params[\"prob\"] = self.prior.prob(masses)\n",
      "/tmp/ipykernel_2281439/3697181178.py:43: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  params[\"prob\"] = self.prior.prob(masses)\n",
      "/tmp/ipykernel_2281439/3697181178.py:43: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  params[\"prob\"] = self.prior.prob(masses)\n",
      "/tmp/ipykernel_2281439/3697181178.py:43: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  params[\"prob\"] = self.prior.prob(masses)\n",
      "/tmp/ipykernel_2281439/3697181178.py:43: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  params[\"prob\"] = self.prior.prob(masses)\n",
      "/tmp/ipykernel_2281439/3697181178.py:43: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  params[\"prob\"] = self.prior.prob(masses)\n",
      " 64%|████████████████████████████████████▌                    | 64623/100630 [3:13:45<1:46:58,  5.61it/s]"
     ]
    }
   ],
   "source": [
    "spacing = 16\n",
    "f = h5py.File(\"data/signals.hdf5\", \"a\")\n",
    "ex = ProcessPoolExecutor(16)\n",
    "try:\n",
    "    try:\n",
    "        split = f[\"test\"]\n",
    "    except KeyError:\n",
    "        split = f.create_group(\"test\")\n",
    "\n",
    "    futures = {}\n",
    "    for start, stop in background_splits[\"test\"]:\n",
    "        duration = stop - start\n",
    "        key = f\"{start}-{duration}\"\n",
    "        try:\n",
    "            segment = split[key]\n",
    "        except KeyError:\n",
    "            segment = split.create_group(key)\n",
    "\n",
    "        for shift in range(1, shifts_per_ifo + 1):\n",
    "            end = stop - shift\n",
    "            for j in range(2):\n",
    "                if len(futures) == num_shifts:\n",
    "                    break\n",
    "                shift_key = [0, shift] if not j else [shift, 0]\n",
    "                shift_key = str(shift_key)\n",
    "                try:\n",
    "                    group = segment[shift_key]\n",
    "                except KeyError:\n",
    "                    group = segment.create_group(shift_key)\n",
    "\n",
    "                num_signals = int((end - start) // spacing) - 1\n",
    "                timestamps = np.arange(num_signals)\n",
    "                timestamps = start + spacing / 2 + timestamps * spacing\n",
    "                shape = (len(timestamps), sampler.max_size)\n",
    "\n",
    "                for ifo in IFOS:\n",
    "                    group.create_dataset(ifo, shape=shape)\n",
    "\n",
    "                group.attrs[\"num_rejected\"] = 0\n",
    "                params = group.create_group(\"parameters\")\n",
    "                params[\"gps_time\"] = timestamps\n",
    "\n",
    "                for i, t_gps in enumerate(timestamps):\n",
    "                    future = ex.submit(sampler, t_gps)\n",
    "                    futures[future] = (key, shift_key, i)\n",
    "\n",
    "    # write them to disk as they come in\n",
    "    fs = list(futures.keys())\n",
    "    for future in tqdm(as_completed(fs), total=len(fs)):\n",
    "        key, shift, idx = futures.pop(future)\n",
    "        waveforms, p, num_rejected = future.result()\n",
    "\n",
    "        dataset = split[key][shift]\n",
    "        for ifo in IFOS:\n",
    "            dataset[ifo][idx] = waveforms[ifo]\n",
    "        dataset.attrs[\"num_rejected\"] += num_rejected\n",
    "\n",
    "        params = dataset[\"parameters\"]\n",
    "        size = len(params[\"gps_time\"])\n",
    "        for k, v in p.items():\n",
    "            try:\n",
    "                dset = params[k]\n",
    "            except KeyError:\n",
    "                dset = params.create_dataset(k, shape=(size,))\n",
    "            dset[idx] = v\n",
    "\n",
    "        if (idx + 1) == size:\n",
    "            f.flush()\n",
    "except Exception:\n",
    "    ex.shutdown(wait=False, cancel_futures=True)\n",
    "    raise\n",
    "else:\n",
    "    ex.shutdown(wait=True, cancel_futures=False)\n",
    "finally:\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
